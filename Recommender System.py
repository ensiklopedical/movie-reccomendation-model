# -*- coding: utf-8 -*-
"""draft 1_Recommender_System_Movies-Faisal_Ahmad_Gifari.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qMbSPFC7xn740J9bXid9UaKBHxTnLrQH

# Proyek Machine Learning - Recommender System

# Data Diri

- Nama : Faisal Ahmad Gifari
- Jenis Kelamin : Laki-Laki
- Pekerjaaan : Mahasiswa
- Tempat/Tanggal Lahir : Kuningan, 17 September 2002
- Username : faisal_ag_037
- email : pd-20379543@edu.jakarta.go.id
- No. Telepon : 085775063559
- Kota Domisili : Jakarta Barat
- Institusi : UIN Syarif Hidayatullah Jakarta

# Importing Library
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow
from keras import Model
from keras.callbacks import EarlyStopping, ModelCheckpoint
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from zipfile import ZipFile
from tensorflow import keras
from keras import layers
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""# Importing Datasets"""

from google.colab import drive
drive.mount('/content/drive')

movie_df = pd.read_csv('/content//drive/MyDrive/Datasets/movies.csv')
review_df = pd.read_csv('/content/drive/MyDrive/Datasets/ratings.csv')

"""Kedua sudah berhasil di-import kedalam notebook.

- `movies.csv` menjadi dataframe dengan nama `movie_df`
- `reviews.csv` menjadi dataframe dengan nama `review_df`
"""

movie_df.head()

review_df.head()

"""Kedua dataframe telah berhasil dibuat.

# Data Undestanding

Dataset yang digunakan untuk pembuatan model system recommendation ini adalah dataset "MovieLens Latest" yang tersedia di situs [grouplens](https://grouplens.org/datasets/movielens/) yang berisi data-data mengenai film-film beserta rating yang diberikan oleh para pengguna. Dataset ini terakhir di-update pada September 2018

Terdapat banyak file didalamnya, tetapi yang digunakan hanya dataset `movie.csv` dan `rating.csv`. `movie.csv` terdiri dari 9078 baris data dan 3 kolom data. Kemudian, `rating.csv` terdiri dari 100836 baris data dan 4 kolom data.

Kedua dataset tersebut dapat digunakan untuk membuat system recommendation, baik `Content-Based Filtering` maupun `Collaborative Filtering`

Dataset tersebut dapat diunduh [disini](https://grouplens.org/datasets/movielens/)

[Direct download](https://files.grouplens.org/datasets/movielens/ml-latest-small.zip)

Berikut ini adalah infomasi lainnya mengenai atribut-atribut yang terdapat pada dua dataset tersebut:


Atribut-atribut pada `movie.csv`:
- ```movieId```: Id film
- ```title```: Judul film
- ```genres```: Genre film


Atribut-atribut pada `rating.csv` :
- ```userId```: Id user
- ```movieId```: Id film
- ```rating```: Skor rating yang sebuah film
- ```timestamp```: Waktu kapan film diberikan skor rating

## Exploratory Data Analysis

Exploratory Data Analysis (EDA) adalah pendekatan analisis data yang bertujuan untuk memahami karakteristik utama dari kumpulan data. EDA melibatkan penggunaan teknik statistik dan visualisasi grafis untuk menemukan pola, hubungan, atau anomali untuk membentuk hipotesis. Proses ini sering kali tidak terstruktur dan dianggap sebagai langkah awal penting dalam analisis data yang membantu menentukan arah analisis lebih lanjut.

### movie_df
"""

# Menampilan jumlah baris dan kolom yang ada pada dataset

movie_df.shape

"""Berdasarkan output diatas, `movie_df` memiliki:
- 9742 baris data
- 3 kolom data
"""

# Menampilkan kolom-kolom yang ada pada dataset

movie_df.keys()

"""Berdasarkan output diatas, `movie_df` memiliki 3 kolom berbeda, yaitu:
- `movieId`
- `title`
- `genres`
"""

# Menampilkan tipe data dari setiap kolom yang ada

movie_df.info()

"""Berdasarkan output diatas, `movie_df` memiliki 3 kolom tersebut memiliki tipe datanya masing-masing, yaitu:
- `movieId` = `int64`
- `title` = `object`
- `genres` = `object`
"""

# Menampilkann total unique value di kolom movieId

print(movie_df['movieId'].nunique())

# Menampilkan total unique value di kolom genre dan mencetak setiap value-nya

unique_genres = movie_df['genres'].unique()
for genre in unique_genres:
    print(genre)

"""Berdasarkan output diatas, `movie_df` memiliki terlalu banyak `genre` nyaris untuk setiap film-nya. Hal ini perlu disimplifikasikan untuk memudahkan tahap-tahap lainnya."""

movie_df['genres'] = movie_df['genres'].str.split('|').str[0]

"""Kode diatas membuat kolom 'genre' hanya mempertahankan genre yang berada padaa urutan pertama saja. Hal ini mempermudah untuk pemrosesan dataset pada tahap-tahap selanjutnya."""

# Menampilkan total unique value di kolom genre dan mencetak setiap value-nya

unique_genres = movie_df['genres'].unique()
for genre in unique_genres:
    print(genre)

movie_df.head()

"""Proses perubahan nilai pada kolom `genre` sudah berhasil dilakukan.


"""

movie_df.rename(columns={'genres': 'genre'}, inplace=True)

"""Nama kolom `genres` berhasil diganti menjadi `genre` karena genre dari tiap film sudah tidak ada yang lebih dari 1

Namun, Masih ada beberapa tindakan yang perlu dilakukan untuk membersihkan `movie_df`. Proses pembersihan dan persiapan dataset akan dikerjakan lebih lanjut pada tahap selanjutnya.

### review_df
"""

# Menampilan jumlah baris dan kolom yang ada pada dataset

review_df.shape

"""Berdasarkan output diatas, `review_df` memiliki:
- 100836 baris data
- 4 kolom data
"""

# Menampilkan kolom-kolom yang ada pada dataset

review_df.keys()

"""Berdasarkan output diatas, `review_df` memiliki 4 kolom berbeda, yaitu:
- `userId`
- `movieId`
- `review`
- `timestamp`
"""

review_df.rename(columns={'rating': 'review'}, inplace=True)

"""Tahap diatas adalah pengubahan nama kolom yang sebelumnya `review` menjadi `review` untuk memudahkan karena sesuai dengan nama dataframe-nya yaitu `review_df`"""

# Menampilkan kolom-kolom yang ada pada dataset

review_df.keys()

"""Berdasarkan output diatas, proses pengubahan nama kolom `review` menjadi `review` telah berhasil dilakukan"""

# Menampilkan tipe data dari setiap kolom yang ada

review_df.info()

"""Berdasarkan output diatas, `review_df` memiliki 4 kolom berbeda dengan tipe datanya masing-masing, yaitu:
- `userId` = `int64`
- `movieId` = `int64`
- `review` = `float64`
- `timestamp` = `int64`
"""

review_df['review'].describe()

"""Fungsi diatas memberikan informasi statistika deskriptif untuk kolom `review`, yaitu:
- ```count``` : Jumlah data dari sebuah kolom
- ```mean``` : Rata-rata dari sebuah kolom
- ```std``` : Standar deviasi dari sebuah kolom
- ```min``` : Nilai terendah pada sebuah kolom
- ```25%``` : Nilai kuartil pertama (Q1) dari sebuah kolom
- ```50%``` : Nilai kuartil kedua (Q2) atau median atau nilai tengah dari sebuah kolom
- ```75%``` : Nilai kuartil ketiha (Q3) dari sebuah kolom
- ```max``` : Nilai tertinggi pada sebuah kolom

Walaupun kolom selain `review` ada yang tetap bisa diproses menggunakan fungsi `describe()` karena bertipe data `int64` dan `float64`, tetapi yang benar-benar kolom numerik hanyalah kolom `review`.
"""

# Menampilakn total unique value di kolom movieId

print(review_df['movieId'].nunique())

"""Berdasarkan output diatas, `review_df` memiliki 9724 `movieId` secara unique dari keseluruhan dataset."""

# Menampilakn total unique value di kolom user

print(review_df['userId'].nunique())

"""Berdasarkan output diatas, `review_df` memiliki 610 `userId` secara unique dari keseluruhan dataset. Hal ini berarti ada 610 user yang memberikan review terhadap film-film yang mereka telah tonton.

## Data Visualization

Visualisasi data adalah proses representasi grafis dari informasi dan data. Dengan menggunakan elemen visual seperti grafik, diagram, dan peta, visualisasi data menyediakan cara yang intuitif dan mudah diakses untuk melihat dan memahami tren, anomali, dan pola dalam data. Tujuan utama dari visualisasi data adalah untuk mengkomunikasikan informasi secara jelas dan efisien kepada pengguna, sehingga memudahkan pemahaman, analisis, dan pengambilan keputusan berdasarkan data tersebut

### movie_df

Univariate Analysis <br>

Univariate Analysis adalah jenis analisis data yang memeriksa satu variabel (atau bidang data) pada satu waktu. Tujuannya adalah untuk menggambarkan data dan menemukan pola yang ada dalam distribusi variabel tersebut. Ini termasuk penggunaan statistik deskriptif, histogram, dan box plots untuk menganalisis distribusi dan memahami sifat dari variabel tersebut.
"""

plt.figure(figsize=(10, 6))
genre_counts = movie_df['genre'].value_counts()
genre_counts_sorted = genre_counts.sort_values(ascending=False)
sns.countplot(x='genre', data=movie_df, color='#30D5C8', order=genre_counts_sorted.index)
sns.despine()
plt.title('Count Plot dari Genre Film')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

# Calculate the percentage of each genre
genre_percentages = genre_counts_sorted / genre_counts_sorted.sum() * 100

# Filter genres with a percentage below 1%
low_percentage_genres = genre_percentages[genre_percentages < 1]

# Combine low percentage genres as 'genre lainnya'
genre_percentages['genre lainnya'] = low_percentage_genres.sum()

# Remove low percentage genres from the original series
genre_percentages = genre_percentages.drop(low_percentage_genres.index)

# Plot the pie chart
plt.figure(figsize=(10, 6))
plt.pie(genre_percentages, labels=genre_percentages.index, autopct='%1.1f%%')
plt.title('Persentase untuk setiap Genre')
plt.show()

"""Berdasarkan visualisasi data diatas, terlihat bahwa `Comedy`, `Drama`, dan `Action` memiliki proporsi dan jumlah terbesar secara keseluruhan dibandingkan genre lainnya pada `movie_df`

Namun, ada beberapa genre yang terlihat terlampau sedikit. Hal ini perlu untuk dicek lebih lanjut.

### review_df

Univariate Analysis

Univariate Analysis <br>

Univariate Analysis adalah jenis analisis data yang memeriksa satu variabel (atau bidang data) pada satu waktu. Tujuannya adalah untuk menggambarkan data dan menemukan pola yang ada dalam distribusi variabel tersebut. Ini termasuk penggunaan statistik deskriptif, histogram, dan box plots untuk menganalisis distribusi dan memahami sifat dari variabel tersebut.
"""

plt.figure(figsize=(10, 6))
review_counts = review_df['review'].value_counts()
review_counts_sorted = review_counts.sort_values(ascending=False)
sns.countplot(x='review', data=review_df, color='#30D5C8', order=review_counts_sorted.index)
sns.despine()
plt.title('Count Plot dari Review Film')
plt.xlabel('Review')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

review_percentages = review_df['review'].value_counts(normalize=True) * 100
plt.figure(figsize=(8, 6))
review_percentages.plot.pie(autopct='%1.1f%%')
plt.title('Percentage of Each Review Value')
plt.ylabel('')
plt.show()

"""Berdasarkan visualisasi data diatas, terlihat bahwa `4.0` dan `3.0` memiliki proporsi dan jumlah terbesar secara keseluruhan dibandingkan skor lainnya pada `review_df`."""

top_10_movies = review_df['movieId'].value_counts().head(10)
top_10_movies.plot(kind='bar')
plt.title('Top 10 Movies with Most reviews')
plt.xlabel('Movie ID')
plt.ylabel('Number of reviews')
plt.show()

"""Berdasarkan visualisasi data diatas, berikut adalah daftar `movieId` dengan review terbanyak pada dataset `review_df`."""

top_10_movies_array = top_10_movies.index.to_numpy()
print(top_10_movies_array)

movie_df[movie_df['movieId'].isin(top_10_movies_array)]

"""Output diatas adalah 10 judul dari film-film yang memiliki jumlah review terbanyak.

# Data Preparation

## movie_df
"""

movie_df

"""### Detection and Removal Duplicates

Data duplikat adalah baris data yang sama persis untuk setiap variabel yang ada. Dataset yang digunakan perlu diperiksa juga apakah dataset memiliki data yang sama atau data duplikat. Jika ada, maka data tersebut harus ditangani dengan menghapus data duplikat tersebut.

**Alasan**: Data duplikat perlu didektesi dan dihapus karena jika dibiarkan pada dataset dapat membuat model Anda memiliki bias, sehingga menyebabkan _overfitting_. Dengan kata lain, model memiliki performa akurasi yang baik pada data pelatihan, tetapi buruk pada data baru. Menghapus data duplikat dapat membantu memastikan bahwa model Anda dapat menemukan pola yang ada lebih baik lagi.
"""

# Cek baris duplikat dalam dataset
duplicates_movie = movie_df.duplicated()

# Hitung jumlah baris duplikat
duplicate_count = duplicates_movie.sum()

# Cetak jumlah baris duplikat
print(f"Number of duplicate rows: {duplicate_count}")

"""Berdasarkan hasil tersebut, tidak ditemukan adanya data duplikat, maka tidak ada juga proses penghapusannya.

### Handle Missing Value

_Missing Value_ terjadi ketika variabel atau barus tertentu kekurangan titik data, sehingga menghasilkan informasi yang tidak lengkap. Nilai yang hilang dapat ditangani dengan berbagai cara seperti imputasi (mengisi nilai yang hilang dengan mean, median, modus, dll), atau penghapusan (menghilangkan baris atau kolom yang nilai hilang)

**Alasan**: _Missing Value_ perlu ditangani karena jika dibiarkan dapat berpengaruh ke rendahnya akurasi model yang akan dibuat. Maka dari itu, penting untuk mengatasi missing value secara efisien untuk mendapatkan model _Machine Learning_ yang baik juga.
"""

movie_df.isnull().sum()

"""Berdasarkan output diatas, tidak adanya missing value pada `movie_df`

### Delete Some Data Point

Pada sebuah dataset, ada saatnya beberapa baris data atau kolom perlu dihapus karena satu dan lain hal. Salah satunya agar tidak menghambat proses training dan performa dari sebuah model yang akan dibangun. Ada value pada dataset `movie_df`, khususnya kolom `genre`, yang perlu dihapus karena nama nilainya itu sendiri, yaitu `(no genres listed)'.

**Alasan**: Hal ini perlu dilakukan karena nilai tersebut tidak mewakili genre apapun untuk sebuah film. Jika dibiarkan, ini dapat mempengaruhi performa model yang akan dibuat. Maka dari itu, baris data yang memiliki nilai ini, perlu dihapus
"""

movie_df.drop(movie_df[movie_df['genre'] == '(no genres listed)'].index, inplace=True)

"""`(no genres listed)` sudah dihapus dari kolom `genre`"""

# Menampilkan total unique value di kolom genre dan mencetak setiap value-nya

unique_genres = movie_df['genre'].unique()
for genre in unique_genres:
    print(genre)

"""Berdasarkan outpur diatas, `(no genres listed)` terbukti sudah tidak ada lagi pada dataset"""

value_counts = movie_df['genre'].value_counts()

# Filter the counts to find values that occur less than 6 times
less_than_six = value_counts[value_counts < 6]

# Print the result
print("Values with less than 6 data points:")
print(less_than_six)

"""Berdasarkan output diatas, `War` pada kolom `genre` hanya memiliki 4 data point. Dalam kasus ini, value yang kurang dari 6 data dalam dataset perlu dihilangkana karena tidak dapat digunakan. Hal ini akan ditindak lebih lanjut pada bagian selanjutnya."""

movie_df = movie_df[~movie_df['genre'].str.contains('War')]

"""Berhasil dilakukannya penghapusan data point pada kolon `genre` yang bernilai `War`

### Changing Certain Value

Pada sebuah dataset, ada kalanya beberapa nilai perlu diproses terlebih dahulu agar proses training atau pembuatan model dapat berjalan seperti seharusnya. Salah satunya adalah pengubahan beberapa nilai yang dirasa akan mengganggu jika dibiarkan. Ada value pada dataset `movie_df`, khususnya kolom `genre`, yang perlu diganti namanya, yaitu `Sci-Fi` dan `Film-Noir`.

**Alasan**: Hal ini perlu dilakukan karena jika dibiarkan ketika proses embedding akan terdeteksi sebagai 2 bagian yang berbeda. Maka dari itu, string dari kedua nilai tersebut harus dimodifikasi agar pada saat proses encoding tidak terpecah menjadi 2 bagian berbeda.
"""

movie_df['genre'] = movie_df['genre'].replace({'Sci-Fi': 'SciFi', 'Film-Noir': 'FilmNoir'})

"""nilai `Sci-Fi` dan `Film-Noir` sudah berhasil diubah dengan menghilangkan tanda `-` pada kedua nilai tersebut. Maka dari itu, nilai tersebut sudah siap untuk diproses pada tahap selanjutnya."""

# Menampilkan total unique value di kolom genre dan mencetak setiap value-nya

unique_genres = movie_df['genre'].unique()
for genre in unique_genres:
    print(genre)

"""Berdasarkan output diatas, dapat dilihat bahwa sudah terlihat nilai yang baru saja diubah, yaitu `SciFi` dan `FilmNoir`"""

movie_df.shape

"""Setelah beberapa proses yang sudah dilakukan, maka `movie_df` masih memiliki:
- 9708 baris data
- 3 kolom data

## review_df
"""

review_df

"""### Detection and Removal Duplicates

Data duplikat adalah baris data yang sama persis untuk setiap variabel yang ada. Dataset yang digunakan perlu diperiksa juga apakah dataset memiliki data yang sama atau data duplikat. Jika ada, maka data tersebut harus ditangani dengan menghapus data duplikat tersebut.

**Alasan**: Data duplikat perlu didektesi dan dihapus karena jika dibiarkan pada dataset dapat membuat model Anda memiliki bias, sehingga menyebabkan _overfitting_. Dengan kata lain, model memiliki performa akurasi yang baik pada data pelatihan, tetapi buruk pada data baru. Menghapus data duplikat dapat membantu memastikan bahwa model Anda dapat menemukan pola yang ada lebih baik lagi.
"""

# Cek baris duplikat dalam dataset
duplicates_review = review_df.duplicated()

# Hitung jumlah baris duplikat
duplicate_review = duplicates_movie.sum()

# Cetak jumlah baris duplikat
print(f"Number of duplicate rows: {duplicate_review}")

"""Berdasarkan hasil tersebut, tidak ditemukan adanya data duplikat, maka tidak ada juga proses penghapusannya.

### Handle Missing Value

_Missing Value_ terjadi ketika variabel atau barus tertentu kekurangan titik data, sehingga menghasilkan informasi yang tidak lengkap. Nilai yang hilang dapat ditangani dengan berbagai cara seperti imputasi (mengisi nilai yang hilang dengan mean, median, modus, dll), atau penghapusan (menghilangkan baris atau kolom yang nilai hilang)

**Alasan**: _Missing Value_ perlu ditangani karena jika dibiarkan dapat berpengaruh ke rendahnya akurasi model yang akan dibuat. Maka dari itu, penting untuk mengatasi missing value secara efisien untuk mendapatkan model _Machine Learning_ yang baik juga.
"""

review_df.isnull().sum()

"""Berdasarkan output diatas, tidak adanya missing value pada `review_df`. Maka, tidak perlu dilakukan pengisian pada data hilang.

### Outliers Detection and Removal

_Outliers_ adalah titik data yang secara signifikan berbeda dari sebagian besar data dalam kumpulan data. Outliers dapat muncul karena variasi dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental; dalam beberapa kasus, outliers bisa juga menunjukkan variabilitas yang sebenarnya dalam data. Penting untuk menganalisis outliers karena mereka dapat memiliki pengaruh besar pada hasil analisis statistik.

Outliers adalah titik data yang secara signifikan berbeda dari sebagian besar data dalam kumpulan data. Outliers dapat muncul karena variasi dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental; dalam beberapa kasus, outliers bisa juga menunjukkan variabilitas yang sebenarnya dalam data. Penting untuk menganalisis outliers karena mereka dapat memiliki pengaruh besar pada hasil analisis statistik.

Proses pembersihan outliers menggunakan metode IQR (Interquartile Range) melibatkan beberapa langkah:

- Menghitung Kuartil: Tentukan kuartil pertama (Q1) dan kuartil ketiga (Q3) dari data. Kuartil ini membagi data menjadi empat bagian yang sama.

- Menghitung IQR: Hitung IQR dengan mengurangi Q1 dari Q3:
  $$IQR=Q3−Q1$$

- Menentukan Batas Outliers:

  - Batas bawah untuk outliers:
    $$Q1−1.5×IQR$$

  - Batas atas untuk outliers:
    $$Q3+1.5×IQR$$

- Identifikasi Outliers: Data yang berada di luar batas bawah dan atas ini dianggap sebagai outliers.

Pembersihan Outliers yang teridentifikasi kemudian dapat dibersihkan dari dataset, baik dengan menghapusnya atau melakukan transformasi tertentu.
    
**Alasan**:_Outliers_ perlu dideteksi dan dihapus karena jika dibiarkan dapat merusak hasil analisis statistik pada kumpulan data sehingga menghasilkan performa model yang kurang baik. Selain itu, Mendeteksi dan menghapus _outlier_ dapat membantu meningkatkan performa model _Machine Learning_ menjadi lebih baik.
"""

review_df['review'].describe()

"""Sebelum memulai dengan proses interquartile. Perlu dilihat terlebih dahulu secara sekilas secara statistika deskriptif.

Hanya kolom `review` yang dicek karena hanya kolom tersebut yang tergolong sebagai kolom numeric dan perlu dilakukan pemeriksaan outliers-nya.

Berdasarkan output diatas, terlihbat bahwa nilai terkecil dari `review` adalah `0.5` dan terbesarnya adalah `5.0`. Kedua nilai tersebut masih di ambang wajar untuk sebuah review film. Jadi, tidak ada outliers dan tidak ada penghapus outliers untuk kolom `review`

### Dropping Uneeded Column

Pada bagian ini adalah proses penghapusan kolom yang tidak digunakan untuk proses pembuatan model. Langkah ini diambil berdasarkan asumsi bahwa kolom yang akan dihapus tidak memberikan kontribusi terhadap prediksi yang dibuat oleh model.

**Alasan**: Tahapan ini perlu dilakukan karena kolom yang tidak digunakan cenderung tidak memberikan informasi yang berguna untuk prediksi dan dapat menambah informasi yang tidak perlu ke dalam model. Dengan menghilangkan fitur-fitur ini, kita dapat mengurangi kompleksitas model dan mempercepat waktu pelatihan.
"""

review_df.drop('timestamp', axis=1, inplace=True)

"""Kolom `timestamp` telah berhasil dihapus. Kolom tersebut dihapus karena tidak diperlukan untuk proses pembuatan sistem rekomendasi secara collaborative filtering."""

review_df.shape

"""Berdasarkan output tersebut, maka `review_df` masih memiliki:
- 100836 baris data
- 3 kolom data

### Encoding

Encoding adalah proses konversi informasi dari satu bentuk atau format ke bentuk lain, yang sering kali dilakukan untuk memastikan kompatibilitas dan pemrosesan yang tepat oleh berbagai sistem komputer. Proses ini sangat penting dalam dunia digital, di mana berbagai jenis data, seperti teks, gambar, dan suara, harus diubah menjadi format yang dapat dipahami oleh perangkat keras dan perangkat lunak.


**Alasan:** Tahap ini perlu dilakukan karena Encoding memungkinkan data dari berbagai sumber dan format untuk diubah menjadi format standar yang dapat dipahami dan memastikan bahwa informasi dapat diproses
"""

user_id = review_df['userId'].unique().tolist() # Mengubah userId menjadi list tanpa nilai yang sama
user_to_user = {x: i for i, x in enumerate(user_id)} # Melakukan encoding userId
user_encode_to_user = {i: x for i, x in enumerate(user_id)} # Melakukan proses encoding angka ke ke userId

print('list userId :  ', user_id)
print('encoded userId :  ', user_to_user)
print('encoded angka ke userId :  ', user_encode_to_user)

"""Berdasarkan output diatas, proses encoding untuk `userId` sudah berhasil dilakukan."""

movie_id = review_df['movieId'].unique().tolist() # Mengubah movieId menjadi list tanpa nilai yang sama
movie_to_movie = {x: i for i, x in enumerate(movie_id)} # Melakukan proses encoding movieId
movie_encode_to_movie = {i: x for i, x in enumerate(movie_id)} # Melakukan proses encoding angka ke movieId

print('list movieId :  ', movie_id)
print('encoded movieId :  ', movie_to_movie)
print('encoded angka ke movieId :  ', movie_encode_to_movie)

"""Berdasarkan output diatas, proses encoding untuk `movieId` sudah berhasil dilakukan."""

review_df['user'] = review_df['userId'].map(user_to_user) # Mapping userId ke dataframe user
review_df['movie'] = review_df['movieId'].map(movie_to_movie) # Mapping movieId ke dataframe resto

"""Hasil encoding tadi, di-mapping ke dalam dataframe `review_df` dengan menempati kolom baru untuk masing-masing hasil."""

review_df.head(5)

"""Proses mapping berhasil dilakukan karena sudah terdapat dua kolom baru, yaitu `user` dan `movie`"""

num_users = len(user_to_user) # Mendapatkan jumlah user
num_movie = len(movie_to_movie) # Mendapatkan jumlah review
min_review = min(review_df['review']) # Nilai minimum review
max_review = max(review_df['review']) # Nilai maksimal review

print('total user: {}'.format(num_users))
print('total review: {}'.format(num_movie))
print('MIN review: {}'.format(min_review))
print('MAX review: {}'.format(max_review))

"""Berdasarkan output diatas, dapat dilihat bahwa pada `review_df` terdapat:
- total user: 610
- total review: 9724
- MIN review: 0.5
- MAX review: 5.0

### Train Test Split

Train Test Split adalah metode yang digunakan untuk membagi dataset menjadi dua bagian: satu untuk melatih model (_training set_) dan satu lagi untuk menguji model (_testing set_). Biasanya, data dibagi dengan proporsi tertentu, misalnya 80% untuk training dan 20% untuk testing.

**Alasan**: Proses ini dilakukan agar dapat mengevaluasi kinerja model secara objektif. Dengan memisahkan data uji, kita dapat mengukur seberapa baik model memprediksi data baru yang tidak pernah dilihat sebelumnya, yang merupakan indikator penting dari kemampuan generalisasi model.
"""

review_df = review_df.sample(frac=1, random_state=18)

review_df

"""Berdasarkan output diatas, proses shuffling atau pengacakan pada `review_df` berhasil dilakukan"""

x_df = review_df[['user', 'movie']].values # Membuat variabel x_df untuk mencocokkan data user dan movie menjadi satu value
y_df = review_df['review'].apply(lambda x: (x - min_review) / (max_review - min_review)).values # Membuat variabel y_df untuk membuat review dari hasil

"""Pemisahan `review_df` menjadi dua bagian ke `x_df` dan `y_df` untuk proses Train Test Split berhasil dilakukan."""

# Membagi menjadi 90% data train dan 10% data validasi
train_indices = int(0.9 * review_df.shape[0])
x_train, x_val, y_train, y_val = (
    x_df[:train_indices],
    x_df[train_indices:],
    y_df[:train_indices],
    y_df[train_indices:]
)

"""Proses Train Test Split telah dilakukan ke empat variabel berbebeda dengan komposisi 0.9 untuk train dan 0.1 untuk val. Berikut adalah keempatnya:
- x_train
- x_val
- y_train
- y_val
"""

print("panjang array dari x_train : " + str(len(x_train)))
x_train

print("panjang array dari x_val : " + str(len(x_val)))
x_val

print("panjang array dari y_train : " + str(len(y_train)))
y_train

print("panjang array dari y_val : " + str(len(y_val)))
y_val

"""Berdasarkan keempat output diatas, terbukti bahwa proses Train Test Split telah berhasil dilakukan dan berhasil ditampung pada keempat variabel yang telah dibuat.

# Modelling and Result

## Content-Based Filtering

Content-Based Filtering adalah metode yang digunakan dalam sistem rekomendasi untuk memberikan saran kepada pengguna berdasarkan item-item yang telah mereka sukai atau pilih sebelumnya. Metode ini berfokus pada karakteristik atau konten dari item yang ingin direkomendasikan.

**Kelebihan Content-Based Filtering:**

- **Personalisasi**: Dapat memberikan rekomendasi yang sangat personal karena didasarkan pada preferensi sebelumnya dari pengguna itu sendiri.
- **Transparansi**: Mudah untuk menjelaskan mengapa suatu item direkomendasikan, karena rekomendasi didasarkan pada fitur-fitur item yang telah disukai pengguna.

**Kekurangan Content-Based Filtering:**

- **Keterbatasan Diversifikasi**: Cenderung merekomendasikan item yang mirip dengan yang sudah diketahui pengguna, sehingga kurang memberikan kejutan atau item baru yang berbeda.
- **Ketergantungan pada Konten:** Memerlukan data yang cukup tentang konten item untuk bekerja dengan baik, dan kualitas rekomendasi sangat bergantung pada kualitas deskripsi item tersebut.

Pendekatan ini menggunakan atribut-atribut atau fitur-fitur item untuk menentukan kesamaan antara item yang ada. Dalam konteks proyek ini, content-based filtering akan memberikan rekomendasi film berdasarkan `genre` dari film yang ada. Model akan memberikan rekomendasi film-film yang memiliki genre yang sama.

### Modelling
"""

# Inisialisasi TfidfVectorizer
tf_id = TfidfVectorizer()
tf_id.fit(movie_df['genre'])
tf_id.get_feature_names_out()

"""Output diatas adalah array yang berisi nilai-nilai yang ada pada kolom `genre`"""

tfidf_matrix = tf_id.fit_transform(movie_df['genre'])
tfidf_matrix.shape # Melihat ukuran matrix tfidf

"""Berdasarkan output diatas, dapat dilihat bahwa ukuran matriksnya sebesar 9708 x 18"""

tfidf_matrix.todense() # Mengubah vektor tf-idf yang berbebntuk matriks menggunakan fungsi todense()

"""Berdasarkan output diatas, proses operasi menggunakan `todense()` sudah berhasil dilakukan"""

# Membuat dataframe untuk melihat tf-idf matrix
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf_id.get_feature_names_out(),
    index=movie_df.title
).sample(17, axis=1).sample(7, axis=0)

"""Berdasarkan output diatas, dataframe berhasil dibuat dengan data dari matriks yang sudah dibuat sebelumnya"""

# Proses perhitungan cosine_similarity
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Berdasarkan output diatas, proses perhitungan `cosine_similarity` telah berhasil dilakukan."""

# Membuat dataframe dari variabel cosine_sim
cosine_sim_df = pd.DataFrame(cosine_sim, index=movie_df['title'], columns=movie_df['title'])
print('Ukuran Dataframe : ', cosine_sim_df.shape)

"""Berdasarkan output diatas, proses pembuatan dataframe berhasil dilakukan dan dataframe memiliki ukuran 9708 x 9708."""

# Melihat similarity matrix pada data
cosine_sim_df.sample(5, axis=1).sample(7, axis=0)

"""Output diatas adalah tampilan dari dataframe yang telah dibuat."""

def movie_recommendations(title, similarity_data=cosine_sim_df, items=movie_df[['title', 'genre']], k=5):
    index = similarity_data.loc[:,title].to_numpy().argpartition(range(-1, -k, -1))
    closest_data = similarity_data.columns[index[-1:-(k+2):-1]]
    closest_data = closest_data.drop(title, errors='ignore')

    return pd.DataFrame(closest_data).merge(items).head(k)

"""Function utama yang digunakan untuk pembuatan model Content Based telah berhasil dibuat

### Result
"""

movie_df[movie_df.title.eq('Train to Busan (2016)')]

"""Untuk contoh atau simulasi penggunaan model, kita gunakan `Train to Busan (2016)` yang ber-genre `Action`"""

recommendations_result = movie_recommendations('Train to Busan (2016)')
recommendations_result

"""Berikut ini adalah hasil dari `Top-N Recommendation` menggunakan Content-Based Filterting. Proses penggunaan model berhasil dilakukan dan model dapat memberikan hasil rekomendasi berdasarkan input yang diberikan.

Pada contoh diatas, model berhasil memberikan rekomendasi film yang juga ber-genre `Action` berdasarkan input yang diberikan, yaitu `Train to Busan (2016)` yang juga bergenre `Action`

**Model telah dapat berfungsi dengan baik**.

## Collaborative Filtering

Collaborative Filtering adalah teknik yang digunakan dalam sistem rekomendasi untuk memberikan saran kepada pengguna berdasarkan preferensi atau perilaku pengguna lain yang memiliki kesamaan. Teknik ini mengumpulkan dan menganalisis sejumlah besar informasi tentang perilaku pengguna, aktivitas, atau preferensi dan memprediksi apa yang pengguna akan suka berdasarkan kesamaan dengan pengguna lain.

**Kelebihan Collaborative Filtering:**

- **Diversifikasi Rekomendasi**: Dapat memberikan rekomendasi yang beragam karena didasarkan pada preferensi dari banyak pengguna.
- **Tidak Bergantung pada Konten**: Tidak memerlukan pengetahuan tentang konten item, sehingga dapat bekerja dengan item yang memiliki sedikit atau tanpa data konten sama sekali.


**Kekurangan Collaborative Filtering:**

- **Masalah Cold Start**: Sulit untuk memberikan rekomendasi kepada pengguna baru atau untuk item baru yang belum memiliki data interaksi.
- **Scalability**: Dapat menjadi tantangan ketika jumlah pengguna dan item sangat besar karena membutuhkan komputasi yang intensif.
- **Collaborative** Filtering bekerja dengan baik ketika ada cukup data dari pengguna, tetapi bisa menjadi kurang efektif jika data tersebut jarang atau tidak lengkap. Oleh karena itu, sering kali digunakan dalam kombinasi dengan teknik lain untuk meningkatkan kinerja sistem rekomendasi.

### Modelling
"""

class RecommenderNet(Model):

  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size

    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)

    self.movie_embedding = layers.Embedding(
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )

    self.movie_bias = layers.Embedding(num_movie, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    movie_vector = self.movie_embedding(inputs[:, 1])
    movie_bias = self.movie_bias(inputs[:, 1])

    dot_user_movie = tensorflow.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tensorflow.nn.sigmoid(x)

"""Function utama yang digunakan untuk pembuatan model Collaborative Filtering telah berhasil dibuat"""

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model
model.compile(
    loss = keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[keras.metrics.RootMeanSquaredError()]
)

"""Inisiasi model telah berhasil dilakukan"""

early_stopper = EarlyStopping(monitor='val_root_mean_squared_error',
                              patience=5,
                              verbose=1,
                              restore_best_weights=True)

"""Inisiasi Callback Early Stopper yang akan memantau proses training model. Model akan berhenti jika `val_root_mean_squared_error` tidak mengalami penurunan lagi selama 5 epochs. Setelah berhenti, model pada epoch tertentu yang memiliki performa terbaik akan dipertahankan"""

history = model.fit(
          x = x_train,
          y = y_train,
          batch_size = 8,
          epochs = 100,
          callbacks = [early_stopper],
          validation_data = (x_val, y_val)
)

"""Berikut ini hasil proses training yang sudah selesai pada epochs ke-15 yang memiliki :

- `loss` : 0.5912
- `root_mean_squared_error` : 0.1817
- `val_loss` : 0.6025
- `val_root_mean_squared_error` : 0.1948

### Result
"""

user_id = review_df.userId.sample(1).iloc[0]
movie_reviewed_by_user = review_df[review_df.userId == user_id]
movie_not_reviewed = review_df[~review_df['movieId'].isin(movie_reviewed_by_user.movieId.values)]['movieId']
movie_not_reviewed = list(
    set(movie_not_reviewed)
    .intersection(set(movie_to_movie.keys()))
)
movie_not_reviewed = [[movie_to_movie.get(x)] for x in movie_not_reviewed]
user_encoder = user_to_user.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_reviewed), movie_not_reviewed)
)

reviews = model.predict(user_movie_array).flatten()

top_reviews_indices = reviews.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encode_to_movie.get(movie_not_reviewed[x][0]) for x in top_reviews_indices
]

print('List recommendations movie untuk users : {}'.format(user_id))
print('====' * 9)
print('Movie dengan skor review tinggi dari user ')
print('=====' * 8)

top_movie_user = (
    movie_reviewed_by_user.sort_values(
        by = 'review',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genre)

print('====' * 8)
print('Top 10 movie recommendation')
print('====' * 8)

recommended_movie = movie_df[movie_df['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genre)

"""Berikut ini adalah hasil dari `Top-N Recommendation` menggunakan Collaborative Filterting. Proses penggunaan model berhasil dilakukan dan model dapat memberikan hasil rekomendasi berdasarkan review dari user tertentu dan memberikan rekomendasi film lainnya yang cocok untuk user tersebut.

Pada contoh diatas, model berhasil memberikan rekomendasi film untuk user nomor `567` yang pernah memberikan skor review tinggi ke film dan genre:
- `Eraserhead (1977) : Drama`
- `Come and See (Idi i smotri) (1985) : Drama`
- `Jetée, La (1962) : Romance`
- `There Will Be Blood (2007) : Drama`
- `It's Such a Beautiful Day (2012) : Animation`

Model memberikan 10 rekomendasi berupa film dengan genre:
- `Shawshank Redemption, The (1994) : Crime`
- `Rear Window (1954) : Mystery`
- `North by Northwest (1959) : Action`
- `Casablanca (1942) : Drama`
- `Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) : Drama`
- `Citizen Kane (1941) : Drama`
- `Rebecca (1940) : Drama`
- `Notorious (1946) : FilmNoir`
- `To Catch a Thief (1955) : Crime`
- `Lawrence of Arabia (1962) : Adventure`

**Model telah dapat berfungsi dengan cukup baik**.

# Evaluation

Untuk mengukur bagaimana performa dari model yang telah dibuat, diperlukannya metriks evaluasi untuk mengevaluasi model sistem rekomendasi film. Berikut adalah rincian metrik yang digunakan untuk tiap pendekatan:

- `Content-Based Filtering` : `Precision`
- `Collaborative Filtering` : `Root Mean Squared Error`

Berikut ini adalah penjelasan mengenai setiap metrik beserta hasil perhitungan metrik dari model yang telah dibuat :

- `Content-Based Filtering` : `Precision`
  - `Precision`
  
    Presisi merupakan ukuran yang menilai efektivitas model klasifikasi dalam mengidentifikasi label positif. Ukuran ini merupakan perbandingan antara jumlah prediksi yang benar-benar positif dengan keseluruhan hasil yang diprediksi sebagai positif, termasuk yang sebenarnya negatif.

    Berikut adalah formula dan cara kerja dari `Precision` :
    
    - Formula

      Precision = TP/(TP+FP)

      Dalam Konteks sistem rekomendasi menjadi:

      ![Precision](https://github.com/ensiklopedical/system-recommendation/assets/115972304/efd048df-2997-4808-addc-da64f4d34469)
      
    - Cara Kerja

      Formula tersebut mengukur presisi dalam konteks sistem rekomendasi. Presisi dihitung dengan membagi jumlah rekomendasi yang relevan dengan jumlah total item yang direkomendasikan. Jadi, jika sebuah sistem merekomendasikan 10 film dan hanya 6 yang relevan atau disukai oleh pengguna, maka presisi sistem tersebut adalah 0.6 atau 60%. Ini menunjukkan seberapa akurat sistem dalam memberikan rekomendasi yang sesuai dengan kebutuhan atau selera pengguna.

- `Colaborative Filtering` : `Root Mean Squared Error`
  - `Root Mean Squared Error`
  
    Root Mean Square Error (RMSE) adalah metrik yang sering digunakan dalam machine learning untuk mengukur seberapa baik sebuah model prediktif dapat memperkirakan nilai yang sebenarnya. RMSE merupakan akar kuadrat dari rata-rata perbedaan kuadrat antara nilai yang diprediksi oleh model dan nilai yang sebenarnya (nilai aktual).

    Berikut ini adalah formula dan cara kerja dari `Root Mean Squared Error` :

    - Formula
   
      RMSE = sqrt [(Σ(Pi – Oi)²) / n]
   
    - Cara Kerja
    
      RMSE menghitung akar kuadrat dari rata-rata perbedaan kuadrat antara nilai yang diprediksi oleh model dan nilai sebenarnya. Proses kerjanya melibatkan beberapa langkah. Pertama, untuk setiap titik data, kita menghitung selisih antara prediksi model dan nilai aktual. Selisih ini kemudian dikuadratkan untuk menghilangkan nilai negatif dan memberikan bobot lebih pada kesalahan yang lebih besar. Setelah itu, kita menghitung rata-rata dari nilai-nilai kuadrat tersebut. Terakhir, kita mengambil akar kuadrat dari rata-rata ini untuk mendapatkan RMSE.

## Content-Based Filtering
"""

# Calculate precision based on title and genre
def calculate_precision(title, genre):
    title_genre_movies = movie_df[(movie_df['title'] == title) & (movie_df['genre'] == genre)]
    recommended_movies = movie_recommendations(title)
    relevant_movies = recommended_movies[(recommended_movies['genre'] == genre)]
    precision = len(relevant_movies['genre'] == genre) / len(recommended_movies['genre'] == genre)

    return precision

"""Function utama yang digunakan untuk menghitung skor `Precision` dari model Content-Based Filtering telah berhasil dibuat."""

genre_movies_df = movie_df.groupby('genre').first().reset_index()[['genre', 'title']]
genre_movies_df

"""Dataframe diatas adalah datafram yang digunakan untuk mengecek skor Presisi untuk setiap rekomendasi dari tiap `genre`. Dataframe tersebut berisi pasangan `title` dan `genre` dari tiap `genre`."""

unique_genres = movie_df['genre'].unique()
unique_genres

"""Berdasarkan hasil diatas, `unique_genre` menampung array yang berisi setiap `genre` yang ada."""

precision_results = []

for index, row in genre_movies_df.iterrows():
    genre = row['genre']
    title = row['title']
    precision = calculate_precision(title, genre)
    precision_results.append((precision, genre))  # Append precision and genre as a tuple

for result in precision_results:
    print(f"Precision: {result[0]}, Genre: {result[1]}")

"""Berdasarkan hasil diatas, model memiliki skor presisi sebesar `1.0` atau `100%` untuk memberikan rekomendasi berdasarkan `genre`.
<br>
**Model memiliki performa yang sangat baik dalam memberikan rekomendasi secara Content-Based Filtering.**

## Collaborative Filtering
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""Berdasarkan plot tersebut, proses training model berhenti pada epoch ke 15 (epochs 1 dimulai dari nomor 0 pada plot) karena `callbacks` yang berisi `early stopper`. `early stopper` menghentikan proses training karena model tidak menunjukkan penurunan yang lebih keci dari `val_root_mean_squared_error` pada epochs ke-15 selama 5 epochs berturut-turut.

Kemudian, model pada epochs ke 15 yang dipertahankan karena pada epochs tersebut model memiliki performa yang terbaik. Berikut adalah hasil dari metriks pada epocs tersebut:
- `loss` : 0.5912
- `root_mean_squared_error` : 0.1817
- `val_loss` : 0.6025
- `val_root_mean_squared_error` : 0.1948
"""